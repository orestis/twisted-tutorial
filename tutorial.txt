================
Twisted Tutorial
================

Network I/O 101
===============

Twisted is a library that can help you easily build networked applications in Python. While Twisted manages to abstract away most of the low-level network details, having at least a high-level understanding of network I/O will be very helpful.

At the very basic level, when you connect two processes using a mechanism that allows them to exchange data, you have networked them. It helps imagining two physical machines connected via a piece wire (FOOTNOTE: this metaphor doesn't stand for some mechanisms like UDP, but let's ignore that for now).

Once two machines are connected via a piece of wire, they can start using it. A single machine can write data at its end of the wire, and after some time it will arrive at the other end of the wire. Also, at any time data may arrive on the same end of the wire.

Event-driven programming
------------------------

We've already found one complication of network programming - data may arrive at any time. So some kind of mechanism must be in place in order to read the data and forward it to interested parties.

This is a form of event-driven programming, similar to GUI programming. GUI programs wait for a user to generate events such as "mouse clicked" or "key pressed" and will run specific methods to handle those events. When doing network programming, the events are streams of bytes arriving at your end of the wire.

The usual way to deal with events arrinving at any possible time is to have a constant running loop that will periodically check if there is a new event and forward it to whoever might be listening. In GUI programs the OS maintains a queue of events and your program will have a UI loop that pulls events off this queue.

Blocking I/O
------------

Unfortunately, network programs have one extra bit of complication. I f you try to read data off a socket (that's the end of the wire) that has no data in it, your program will suspend execution until some data arrives. For small programs that just need to do one thing, this is not a problem. However, when you have a long-running process, such a web server, that needs to stay responsive to other clients, this becomes an issue.

The usual way of avoiding suspening execution, or "blocking", is to put independent socket operations in different threads or even processes. This way, one can have a central service that just handles new connections, but the actual data back-and-forth happens in parallel. 

This approach, while completely correct and proven over the years, has some drawbacks:

* Every thread or process launched will consume some amount of memory. This means that the number of concurrent connections is limited by the available memory size.
* Writing large multi-threaded programs is hard. Avoiding starvation, deadlocks, guarding access to shared data with locks, synchronisation etc is very hard to get right and even harder to debug.
* Writing multi-process programs means that you have to use some kind of inter-process communication to pass data back and forth, which has a certain overhead.

Non-blocking I/O
----------------

The alternative approach is to open the socket in a non-blocking mode. This way, before you commit to read or write to a socket, you can check to see if it is ready. If it is, the OS guarantees that the operation will not block. If it isn't, you just check again a bit later. 

This sounds like a very good solution, and indeed you can implement it yourself using a good socket library. Unfortunately, there are a lot of things that need to be done just in the right way to ensure that no network operations will ever block, and if you need to support multiple platforms then there are subtle edge cases you need to worry about.

Fortunately, Twisted has solved all those problems and will give you non-blocking network I/O on most major platforms for free, leaving you free to focus on your actual application instead of worrying about low-level network details (FOOTNOTE: All abstractions are leaky. An understanding of the basics of networks can be very beneficial). Twisted provides an event loop that will forward data arriving from the network to interested handlers, and will also take care of sending data over the network to remote machines. This event loop all runs in a single thread, removing any reason to worry about synchronising data access or any other threading issues.

My first server
===============

Let's see an actual example of this. We'll write a server that listens on a TCP port, allows multiple clients to connect at the same time, and will print all data to its standard output.

First we need to setup the event loop. In Twisted we do this by using a global object called "reactor":

..code:
    from twisted.internet import reactor
    reactor.run()

When you run the snippet above, you will see that your program goes into an infinite loop, because the reactor loop has taken over. You will need to kill your process with ctrl-c to get your shell back.

Defining behaviour
------------------

The next step is to install a network event handler. The first event we care about is someone connecting to our server. When this happens, we should create a separate instance of a class for each connection, to ensure client-specific state is separated (if we were doing this with threads, we'd use a threadlocal object. However Twisted is single-threaded, so we can use plain Object Oriented Programming approaches). The classes that we'll be creating must implement twisted.internet.interfaces.IProtocol.

The design pattern of something that creates instances is called a Factory, so our "connection made" handler will be a twisted.internet.interfaces.IProtocolFactory. 

Twisted fortunately has concrete implementations of those interfaces. So we will be using twisted.internet.protocol.Factory and we'll subclass twisted.internet.protocol.Protocol to provide our custom behaviour.

(FOOTNOTE: While Twisted has default implementations for those interfaces, you are free to implement your own should you ever need to. Just make sure you conform to the interface. It usually is a better idea to subclass BaseProtocol than to start from scratch.)

.. code: helloworld.py 1:10

So, at first we do the necessary imports. Then we define a very simple protocol, that will print whatever data arrives in its standard output. Then we define a factory that on every connection will create instances of the HelloWorldProtocol. Note how none of this knows or cares of where the data comes from. Twisted will deal with creating instances of the protocol as new connections arrive, and will call 'dataReceived' whenever data becomes available.

(SIDENOTE: `dataReceived` will be called with arbitrary amounts of data. The protocol class is required to buffer incoming data and try to divide them in meaningful messages. The best way is to assume you will receive data one byte at a time. That said, Twisted has a lot of useful protocol subclasses that help with usual message formats such as line-separated, length prefixed etc. See `twisted.protocols` for a complete list.)

Listening on a port
-------------------

The final piece is installing the factory. Since we want a TCP server, we will attach it to a specific TCP port. To do this, we need to use something called an Endpoint.

Endpoints are a bit like our end of the wire. You create an endpoint of a specific type - in our case a TCP endpoint - with transport-specific paramters - in our case, a port number. Then you attach a factory instance to it.

.. code: helloworld.py 11:17

So, we start again with necessary imports. We then create an endpoint instance and a factory instance and bind them together. Finally we start the reactor loop.

(SIDENOTE: While the reactor is used like a global object, new code is advised to accept reactor as a parameter. This allows for much better testing, passing special-behavior reactors, and in the future will allow for multiple reactors running together)

(SIDENOTE: Endpoints are a fairly recent (in Twisted time) API. If you have an older version of Twisted, you will need to use the equivalent but less flexible approach of doing `reactor.listenTCP(8000, factory)`.)

Trying it out
-------------

Try running the complete listing on a shell (watch out for firewalls blocking access!) and from another shell do `telnet 127.0.0.1 8000`. Start typing data into telnet (telnet will only send them across the wire when you hit enter - do not rely on this behaviour!). Kill the telnet process by sending the escape character (usually `ctrl-]`) followed by `ctrl-c`. Try starting multiple parallel telnet sessions. You'll find that the server quite happily accepts multiple connections.
`

Sending data back to the client
===============================

We've already made a huge step - we wrote a non-blocking server that accepts data from multiple clients in only a few lines of code. While this may be useful if you want to do things like logging, at most cases you'd want to send something back to the client.

The way to send data to the client is by using the `transport` attribute on our protocol instance. This is an implementation of `twisted.internet.interfaces.ITransport` we can think about it as the end of the wire you write data into. This happens by using `write` or `writeSequence`.

So, let's extend our server to make it send every piece of data it receives back to the client, but uppercased! We do this in our `dataReceived` handler.

..code: upperserver.py 3:9

So just like that we can send data back to the client.

Something a bit more useful
===========================

So far we've covered the very basics of writing a server. However, even with this small knowledge we can go very very far. We will now write an almost-compliant memcached server that you can use on your webserver with any compatible memcached client.

The basic stuff
---------------

At its core, memcached is a key-value store. So our implementation will be based on a plain python dictionary.

.. code: memcached1.py

Note we are ignoring the timeout value for simplicity. By the end of this tutorial we'll have built a truly compliant memcached server.

Sharing state
-------------

Memcached is useful because multiple clients can connect to it and shared the same data. Therefore, we will need to create a single instance of `Memcached` and share it between different protocol instances. We do this by passing a reference to it when we build a protocol instance in our factory.

.. code: memcached_server.py 53:63

Our Factory now has an `__init__` method, and we also override the `buildProtocol` method to create a protocol instance with specific arguments.

(SIDENOTE: You will find a lot of Twisted examples or code that uses a slightly different approach of relying on the default implementation of `twisted.internet.protocol.Factory` that sets a `factory` attribute on all protocol instances it creates, pointing to itself. This way a protocol instance can access shared state by doing `self.factory.store`. While this is normal python usage, newcomers tend to be confused and assume some magic is going on. It is recommended that a Protocol class accepts any dependencies as arguments for code clarity and of course testability.)

Specification
-------------

This factory will not change throughout the tutorial. Let's see the protocol next. We should start at the memcached protocol spefication (actually, a subset of it - get and set commands).

A clients sends a 'get' command by sending the following string of bytes: `get <key>\r\n` where the key is a string. A 'set' command is initiated by first sending `set <key> <flags> <timeout> <length>\r\n` where flags is an integer that can be retrieved later, timeout is the number of seconds this entry will stay around and length is the length of the data that will represent the value. Then the client sends <length>number of bytes, followed by `\r\n`. Keys may not contain spaces or newlines, and must be less than 250 characters.

Example:

.. pre:

    set example_key 0 3600 13\r\n
    example_value\r\n

(line break added for clarity)

Decoding messages
-----------------

Remember how `dataReceived` is called with any data that is available and it is the protocol's job to decode that into actual messages? In this case, it seems that splitting at lines is important. We could do it ourselves by buffering the data and looking for new line delimiters, but this a so frequent example that Twisted has already support for it, in the form of `twisted.protocols.LineReceiver`. This is a Protocol subclass that instead of dataReceived exposes `lineReceived` method that will only be called with complete lines (without the delimiter).

.. code: memcached_server.py 1:15

Much easier - now lineReceived just parses the arguments and delegates ot other methods to do the actual work.

The spec for responding to a `get` request is straightforward. If the requested key is in the store, we respond with `VALUE <key> <flags> <length>\r\n`, then the raw data followed by '\r\n' then `END\r\n`. If the key is not in the store, just send `END\r\n`.

.. code: memcached_server.py 16:25

So, this is exactly what we do. We use the `sendLine` convenience method of `basic.LineReceiver` when we need to have `\r\n` delimiters, and `self.transport.write` when we need to send raw data.

The spec for responding to a `set` request is a bit more involved. This is because the client will send `set <key> <flags> <timeout> <length>\r\n` followed by `length` bytes of data followed by `\r\n`. Since the raw data *can* contain newlines or indeed any binary data, we cannot use `lineReceived` as it will try to parse them. Fortunately `basic.LineReceiver` has a way to do just that, called `setRawMode`. Calling this will deactivate the line parsing behaviour and data will be delivered instead using the `rawDataReceived` method. When we want to switch the line parsing mode on again, we'll call `setLineMode` passing in any superfluous data we might have received.

When the server has managed to store the value, it will send `STORED` back.

(SIDENOTE: This kind of behaviour is very usual when designing or implementing protocols. Fortunately Twisted has already support for a lot of protocols in `twisted.protocols` and a lot of utility class ready for subclassing in `twisted.protocols.basic`.)

(TODO: find if the client is allowed to send new requests if a `STORED` is pending)

..code: memcached_server.py 27:43

So, when `handle_set` is called the first thing it does is initialise some instance variables that will be passed on to the actual store later on (if this seems a bit unsafe, remember that this protocol instance will handle data only from a single client connection), and then goes into raw data mode.

`rawDataReceived` is then called repeatedly, buffering the received data. Once `<length>` amount of bytes has been received, plus the newline required by the spec, we store everything in the store, reply with 'STORED\r\n' and switch line mode back on. Since we might have received more data than we can handle (for example, the start of a new command) we need to pass that in `setLineMode` so that the line parsing mechanism can pass them into the next call of `lineReceived`.

(SIDENOTE: Notice that so far everything we have been doing for implementing the protocol is not Twisted specific and with a bit of API massaging it could be reused with any similar networking library. In fact, there is a PEP being written currently that hopefully be in Python 3.4 that will provide a library-agnostic API that various protocols can provide so that they can be used with any networking library that supports the Python Async API.)

..code: memcached_server.py 58:64


(TODO: make sure we haven't done anything stupid and very incompatible in the above code)

(TODO: why oh why aren't those classes new-style objects? How is someone supposed to know if something has an __init__ or not?)

Finally we just instantiate an endpoint and a factory and bind them together. We are using the memcached default port, 11211. Run this example and have a play with it using any memcached-compatible client and see it behaves quite as you expected (at least when using only the get and set commands). (WARNING: This implementation is not suitable for production systems and certainly not supposed to be exposed to untrusted clients)

(TODO: perhaps we should point people to an actual production implementation here?)

Adding timeouts
---------------

The implementation, apart from its gaping security holes, has another problem - it ignores the timeout setting completely. This will mean that at some point your server will run out of memory and even worse, clients would expect things to be expiring and they would receive incorrect results. Let's fix that.

If you have a function that needs to be called some time in the future, `reactor.callLater` does this for you. It works like so:

..code:

    from twisted.internet import reactor

    def print_it(s):
        print s
    reactor.callLater(5, print_it, "5 seconds passed")
    reactor.callLater(10, print_it, "10 seconds passed")
    reactor.callLater(11, reactor.stop)

    reactor.run()

If you run the above snippet you'll see that it will print two lines after 5 and 10 seconds respectively, and after 11 seconds it will stop and exit to the command line.

(SIDENOTE: Careful readers may have read that reactor also has a `listenTCP` method, and we now also show `callLater`. The global reactor instance actually implements numerous interfaces, all found in `twisted.internet.reactor`)

When scheduling a call into the future, it is useful to be able to retain a reference to it so you can cancel it or reschedule it. `callLater` returns an `IDelayedCall` instance that you can call `cancel`, `reset` or `delay` on.

..code: 

    def print_it(p):
        print p

    def abort(call):
        if call.active():
            print 'CANCELLING'
            call.cancel()

    delayedCall = reactor.callLater(5, print_it, 'HI')
    reactor.callLater(4, abort, delayedCall)
    reactor.callLater(6, reactor.stop)

    reactor.run()

Running the above snippet you will see how 'HI' is never printed because the call was cancelled. Note also that you can only call `cancel` on a call that is still active. Calling `cancel` on a call that has either been called or has been cancelled will raise an Exception.

So, in our case, we, will change our memcached store implementation to look like this:

..code: memcached2.py

When dealing with calls that will arrive later, we must take care that a left-over timeout will expire a key prematurely, so we cancel any timeouts that may or may not be pending before we schedule any new ones.

All we need to change to support timeouts is we import `Memcached` from `memcached2` instead of `memcached1`.

A summary
---------

So, in bullet form, here's key points covered so far:

* To do anything in Twisted, you need to start a reactor loop by doing `reactor.run()`
* To find a piece of wire to listen on, you need to instantiate an appropriate endpoint.
* To handle new connections, you need a protocol factory that will build protocol instances.
* Protocol instances only deal with a single connection and only live as long as that.
* It is your responsibility to deal with shared state by injecting it into the protocol instances you create. Subclass a server factory.
* Data will arrive in a protocol instance at arbitrary chunks. Test is with a byte at a time to make sure you are not making any false assumptions.
* Implementing protocols correctly is hard, try to reuse as much code as possible.
* To schedule a function call later in time, use `reactor.callLater` and keep a reference to the return value if you want to do something with it before it's called.

Something even cooler - a proxy server
======================================

A memcached server is cool, but let's dive a bit deeper. Let's write a proxy server that will receive a url followed by a newline, it will fetch the url (following any redirects) and return it to the client. After it has finished sending all the data, it must close the connection to the client. Of course it has to support multiple simultaneous connections. Let's also make it print various information messages on its stdout, such as time took for each request.

(SIDENOTE: For reasons that will become apparent soon, you must not do this in Twisted code. If you're looking for an actually working proxy server example code, see `proxy2.py`. If you're looking for an actual HTTP proxy server, see `twisted.web.proxy`)

..code: proxy1.py 6:17

The interesting bit here is the ProxyProtocol. Since we are expecting a url followed by a newline, we are inheriting from `basic.LineReceiver`. For simplicity, we ignore lines not beginning with 'http://', record the start time, use `urllib2.urlopen` to get a file-like object back, then read from it, write the data to the transport, then call `loseConnection`. Twisted will make sure that all pending data writes will be finished before the connection is closed. We finally print the time it took to fetch the url.

Test it with a simple telnet connection, and you will see it works fine. Let's try more than one simultaneous clients. We would expect that clients will receive a response at roughly the time it takes the server to fetch it, perhaps with a little bit of overhead.

If you're following this at home, download `timingclient.py` and run it like so: `python timingclient.py 127.0.0.1` <list of space separated urls>`. Here are some actual results:


Site           Server         Client
=====          ======         ======
A              0.771           3.817
B              0.567           2.026
C              1.457           2.026
D              1.019           3.815
-----          ------         ------
Total:         3.813           3.817

Wait a minute, this looks wrong. We expected the server and the client to take roughly the same amount of time, but it looks very off. If we look closely, it looks like some connections had to wait until other connections were finished, and in fact two of them had to wait for the total time it took the server to finish *all* requests.

Eagle-eyed readers will have noticed the previous sidenote - our code is wrong. The culprit is the `urllib2.urlopen` call. When diving into the actual implementation of that call, we'll find that at its basic level it opens a plain socket to a remote machine and reads data from there. However, as we said at the very start, you cannot have single-threaded programs making blocking network connections, because the reactor loop will stall and will not have the chance to root unrelated data to other handlers. One potential way to deal with this will be to make the call in a different thread, but since this would introduce the usual threading issues and would defeat the purpose of using with an asynchronous I/O library in the first place!

(SIDENOTE: In some cases, like database interactions that cannot be done asynchronously because of the need to support atomic transactions, using threads is the correct approach. For actual database support look at `twisted.enterprise.dbapi`. For general threading support, use `reactor.deferToThread`. More about deferreds soon).

So we have finally hit a snag - there are *a lot* of libraries and tools out there that use blocking sockets by default. This means we cannot use them in Twisted code (unless we do it in a thread - see sidenote). Fortunately Twisted comes with a lot of batteries built-in, and there's a wealth of Twisted-compatible alternatives to choose from. Look for any library that has the `tx` prefix. For a community-maintained listing, look at http://launchpad.net/tx .

In this particular case we need a library that can do HTTP requests without using blocking sockets. Twisted has this built-in in `twisted.web.client.getPage`. This is a utility function that will take a url, connect to it, and return the data some time in the future.

Wait, what?
-----------

Don't panic, we're still using Python and there's no magic way to receive data in the future. What we'll do instead is going to install another network event handler. Similar to how we have a factory handling "connection made" events, protocol instances handling "data received" calls, we'll install a handler for a "requested url fetch finished" event. After all, we're still receiving data over the network and we don't know when it'll arrive.

In fact, `getPage` is implemented by using factories and protocols. A connection is made to a server, a new protocol instance is created that asks the server for a specific resource, and buffers data passed into its `dataReceived` method. When the server signals it's finished sending data, the protocol can then fire the "requested url fetch finished" event to anyone interested, passing the collected data along.

However, while the 'connection made' and 'data received' events are common network events, it'd madness to have a separate API for every possible event that someone can think of. What we have instead is a way to allow defining custom events and attaching arbitrary handlers to them.

So, what the call to `getPage` will return is an instance of a `twisted.internet.defer.Deferred`. A Deferred instance is a specific event that will happen in the future. In this particular case, the event is 'the url you've requested has finished loading', so the event carries some data as well (similar to how `dataReceived` carries data as well, but more high-level as it will return *all* the data and not chunks).

So now that we have an event, we should attach an event handler function to it. In our particular case, the event handler function might look like this:

..code: proxy2.py 15:19

We attach it to the deffered instance by using `addCallback`.

And in context:

..code: proxy2.py 8:21

If you run `proxy2.py` and make the same requests as before, you will get results that look something like this:

Site           Server         Client
=====          ======         ======
A              0.850           0.853
B              0.486           0.488
C              1.582           1.584
D              0.999           1.000
-----          ------         ------
Total:         3.918           1.585

So, each request arrived at the client at roughly the same time it took the server to fetch it, and all requests executed in parallel. So the client finished when the longest request did. This mean all the requests executed in parallel and any network I/O happened in a non-blocking way.

So, to recap - `getPage` will return a `Deferred` instance that represents the 'url fetched' event. We define a function named `urlFetched` that accepts a single argument, `data`. We register this function by calling `deferredData.addCallback(urlFetched)`. When all the data from the url has been fetched, `urlFetched` will be called with the fetched data, and it will write the data to the transport and close the connection. Note that since we are defining this function inside the scope of `lineReceived`, we can access the `url` and `start` variables. In Twisted-speak, `urlFetched` is called a "callback function", hence the `addCallback` method name.

Defining nested functions can become confusing very soon, so `addCallback` can accept arbitrary arguments (both positional and keyword) that will be passed to the callback function. The first argument will always be the data the Deferred carries (even if it is None).

In this case, two argument `urlFetched` requires are `url` and `start` so, let's make it accept it and pass it via the `addCallback` call:

..code: proxy3.py 8:20

A caching proxy server
----------------------

So, we have a reasonable implementation of a proxy server. Let's add another feature, the proxy server should cache responses and return subsequent requests to the same address from the cache.

Let's start the simple way, by having just an in-memory dict that will store an address as the key and the response as the value. Since the cache must be shared, we will inject it to each protocol instance we create by subclassing the factory and defining our own `buildProtocol` method.

(SIDENOTE: You might have expected we'd use the key-value store we implemented at the start of this tutorial - have patience, we need to cover a few important topics first to avoid potential confusion)

Let's first handle the case where we do have the data in the cache:

..code:

    if url in self.cache:
        data = self.cache[url]
        self.urlFetched(data, url, start)

Note how can just call `urlFetched` with the cached data. It is, after all, just a simple python method. Since it implements a well-defined piece of functionality, we can just reuse it.

In the case where we don't have the data in the cache, we need to do three things:

a) call `client.getPage` to initiate the request
b) store the data in the cache
c) call `urlFetched` with the data

We know how to go from a) to b) - just define a `storeInCache` method and attach it as a callback. Going from b) to c) is a bit more tricky. We could just call `urlFetched` from `storeInCache` but it would look messy:

..code:

    def storeInCache(self, data, url, start):
        self.cache[url] = data
        self.urlFetched(data, url, start)

We are passing in extra arguments (`start`) that will just be forwarded to `urlFetched`. A nicer thing to do would be to allow callbacks to be chained together so that one will run after another. This way every callback function can do just one thing (and be reused the same way we reused `urlFetched` when we already had the data available).

Fortunately this is such a common pattern that `Deferred` supports it natively - just do another `addCallback`. To allow for more interesting behaviour, callbacks are required to return a result that will get passed to the next callback and so on. In our particular case, `storeInCache` will need to return the raw data so that `urlFetched` can access them. Of course, if you forget to return something in your callback functions, the next callback will receive `None`.

..code: proxy4.py 16:31

That's better! We have two plain methods that do one specific thing, and we chain them together. There's another little thing we can do to improve this code though - eliminate the duplication of typing `urlFetched` twice. What we should do instead is have a utility function like `getCachedPage(url, cache)` that will contain all the cache logic and will return just a deferred we'll attach `urlFetched` to.

..code: proxy5.py 5:29

That's better! `getCachedPage` is now a reusable function that can be used in other places as well (and of course, being simple and standalone, it can be easily tested as well). Client code doesn't care how storing is done (or even if it will be done at all) - client code just cares about the 'url fetched' event.

You might be wandering that the `defer.succeed` is doing there. Since the `getCachedPage` function needs to return a `Deferred` so that client code can attach callbacks to it without caring where the data came from, we need to wrap actual data in a `Deferred` instance. `defer.succeed` does just that - it's a way to convert an already existing result into a deferred that will trigger its callback chain immediately.

Writing clients
===============

Time to put the memcached server we wrote to good use - we'll use that as our cache layer for the proxy server. This way, we can share the same cache between multiple proxy servers.

In order to do that, we need to somehow connect to the cache server, and speak the expected protocol. Our final aim is to have a `getMemcachedPage` that will do all the background cache checking and return a `Deferred` instance that will carry the data.

For simplicity, we'll implement the different commands as separate protocol classes, and we'll close the connection to the server once we get a response. Let's start with the `get` command protocol. 

(SIDENOTE: Memcached actually encourages you to maintain a single persistent to the server for as long as you can, in order to save the the connect/teardown overhead associated with every new connection. We'll get to that a bit later. Meanwhile, if you're actually looking for a real memcached client implementation, `twisted.protocols.memcached` has what you need)

..code: memcached_client.py 1:31

The first interesting observation is that this protocol has an actual method named `get` that we can call once we are connected. `rawDataReceived` and `lineReceived` look a lot like the server implementation, with one difference - when we receive the `END` line, we know the server has finished responding so we print whatever value we received (may be None) and we close the connection. The factory is very simple as well.

..code: memcached_client.py 33:43

In order to actually talk to a server, we need to create an endpoint. Since this is the client side, we use a `TCP4ClientEndpoint` that takes a reactor, a host name and a port. Then instead of `listen` we call `connect` passing in our factory instance. Notice that this call returns a `Deferred`, since the connection will happen in the future. When a connection has been made, a protocol instance will be created, and passed to our callback function, which will call `get`.

Note how the server version of listening didn't return any Deferred - this is because when listening we accept multiple connections on the same port, and multiple protocol instances will be created. However for cients, every `connect` call corresponds to exactly one protocol instance, so we can get a reference to it and make any interaction we want. A factory is still used to create those instances, of course.

(SIDENOTE: Some older, mostly equivalent APIs are `reactor.connectTCP` and `ClientCreator`. The endpoint API is the preferred one though).

Creating Deferreds
------------------

In the above implementation, we print the value to stdout when we receive it. This is not very useful - we need to give this value to the code that called our `get` method. But, when the `get` method is called we have no value - the value will come in the future. By now you should be guessing that `get` should return a `Deferred` instance that we'll trigger once the server finishes responding.

..code: memcached_client2.py 1:22

So, what `get` does is create a `Deferred` instance (it is of course a plain python class!), keep a reference to it and return it to anyone interested. When we receive a final response, we call `self.deferred.callback`, passing in a tuple of the value and the flags. Calling `callback` will start running the callback chain, passing in the result. Remember that callback functions must accept the result a single first argument, so we need to pass just a single argument to `callback`, hence the need to wrap it in a tuple.

..code: memcached_client2.py 36:51

Finally, now that `get` returns a callback, we can add a callback to it to get the value and flags. Unfortunately there's a bit of nesting going on - we'll fix that in a while.

Note also that we started to do the proper thing and moved any connection specific stuff to happen only when you run the module, because we're going to be importing this module very very soon.

Let's finish things off by implementing the `set` command and making it a bit easier to use.

..code: memcached_client3.py 1:15

This protocol is much simpler - just write out the correct line, the raw data and finish with a new line, then wait for the 'STORED' response.

..code: memcached_client3.py 18:34

Using it is also straightforward - create an endpoint, connect, get the protocol instance, call 'set', wait for the confirmation that it was actually set. Notice that we ignore the returned value (using the common underscore name to indicate we will not do anything with the argument), since the callback passes None anyway. We must accept one argument (and call `callback` with one too) in order to conform to the `Deferred` API.

Notice how we don't even bother to subclass `Factory`, and we just set the protocol class directly onto the instance. This is a very common idiom in Twisted when we don't really need the factory to do anything else than building protocol instances with the default behaviour.

Create an API
-------------

So far we're using those protocols from the command line, but it's time to actually embed them into our proxy server. For that, we'll create a simple utility class that will do the connection for us. Since it's the callers concern where the connection should be made, we'll expect an endpoint as a parameter.

(SIDENOTE: People familiar with the previous Twisted APIs will realise this is a very nice improvement. By passing in different endpoints, we can connect to a server over SSL or Unix sockets or anything that endpoints support, without changing a single line of code.)

..code: memcached_client4.py 1:13

We import the protocols, create a couple of factories (outside the class, since they don't have any kind of state) and define the constructor of our client.

..code: memcached_client4.py 15:35

Now this looks ugly! There are 3 levels of nesting there, and the reason is that we have a chain of callbacks that all have potential "pauses" in them because they are accessing the network, so we need to unravel all the chain to get to the final value. 

It turns out that this is again such a common requirement that Deferred has built-in support for it. All you need to to is return a *new* Deferred instance from a callback function. When this happens, the chain of callbacks is suspended until the result of that new Deferred comes back. When it does, it is passed to the next callback. This will look easier in code:

..code: memcached_client5.py 15:27

This looks much cleaner. Let's see what's going on. First, we connect to the server, getting a deferred back. When that deferred fires, it will give us an instance of the relevant protocol, connected to the server. We can then call `get` or `set` on the protocol. Those calls will also return deferreds, which we just return. This is all transparent to the client:

..code: memcached_client5.py 29:39

Let's walk the execution chain, just as the connection happens:

* connection is established
* `got_protocol` is executed
    * `p.get` is executed, returning a deferred
* Since a callback returned a deferred, the execution pauses here, instead of proceeding to the `got_value` callback the client attached.
* The server finishes responding
* The deferred that paused the execution is called back with the returned value from the server
* `got_value` is executed with the value of the previous callback, which is the return value from the server.

The point to keep is that whenever you need to access the network in one of your callbacks, just return the deferred you get and everything will proceed normally when that is finished.

Let's bring it all together
---------------------------

Remember our proxy server? Let's integrate it with the key-value store. We'll need to change it a bit so that instead of a cache instance protocols will share a Memcached client instance.

We don't need to change very much in our Protocol and Factory - instead of using `getCachedPage` we use `getMemcachedPage`, and instead of creating a dictionary cache, we use a `MemcachedClient`:

..code: proxy6.py 21:48

Other than that, the client is exactly the same.

The `getMemcachedPage` function is a bit more complicated, albeit more interesting:

..code: proxy6.py 5:19

Everything starts by looking into the cache for the request value. As this returns a deferred, since we now have to go to the network, we need to specify what happens when we receive a response in the `got` callback.

If we have a value in the cache, just return it. Otherwise fetch the page. When the page is fetched, store it in the cache. Since the `set` doesn't return any data, add a lambda callback that will return it to the final client code.

(TODO: I think this desperately needs a diagram showing the flow. Perhaps there's a clearer way to write it too?)













Asynchronous Programming 101
============================

So, we've found a way to deal with data coming from a socket in a non-blocking fashion, so we can use an event loop that will forward them to our handlers. However, since 





